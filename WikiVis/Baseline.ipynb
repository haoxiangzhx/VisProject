{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: sentence embeddings+noun embedding+adj embeddding\n",
    "# or named entity embeddings\n",
    "# column edit distance??\n",
    "\n",
    "# https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a\n",
    "\n",
    "# Wiki Pre Trained with Fasttext https://fasttext.cc/docs/en/english-vectors.html\n",
    "# Advances in Pre-Training Distributed Word Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy??\n",
    "# https://spacy.io/\n",
    "import io\n",
    "import json\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    cnt = 0\n",
    "    for line in fin:\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            print(cnt)\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(fname):\n",
    "    with open(fname) as f:\n",
    "        questions_list = f.readlines()\n",
    "\n",
    "    filtered_questions_list = []\n",
    "    for question in questions_list:\n",
    "        question_json = json.loads(question)\n",
    "        question_json.pop('question1')\n",
    "        question_json.pop('question2')\n",
    "        filtered_questions_list.append(question_json)\n",
    "            \n",
    "    return filtered_questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tables(part_file_name):\n",
    "    with open('../WikiSQL/data/'+part_file_name+'.tables.jsonl') as f:\n",
    "        tables_list = f.readlines()\n",
    "        \n",
    "    tables = {}\n",
    "    for table in tables_list:\n",
    "        table_json = json.loads(table)\n",
    "        tables[table_json['id']] = table_json\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, spacy_model, fastText_model):\n",
    "    # weight more on nouns that are not stop words\n",
    "    embed_dim = 300\n",
    "    noun_amplify = 3\n",
    "    doc = nlp(sentence)\n",
    "    embed_res = np.zeros((embed_dim,))\n",
    "    doc_len = len(doc)\n",
    "    noun_cnt = 0\n",
    "    for token in doc:\n",
    "        if token.lemma_ in fastText_model:\n",
    "            if token.pos_ == 'NOUN' and token.is_stop == False:\n",
    "                embed_res += noun_amplify*np.array(fastText_model[token.lemma_])\n",
    "                noun_cnt += 1\n",
    "            else:\n",
    "                embed_res += np.array(fastText_model[token.lemma_])\n",
    "        else:\n",
    "            embed_res += np.zeros((embed_dim,))\n",
    "#         print(fastText_model[token.lemma_][-1], embed_res[-1])\n",
    "    embed_res = np.array(embed_res)/(doc_len+(noun_amplify-1)*noun_cnt)\n",
    "    return embed_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headers_embedding(headers, spacy_model, fastText_model):\n",
    "    embed_dim = 300\n",
    "    header_embedding = np.empty((0, embed_dim))\n",
    "    for col_name in headers:\n",
    "        col_embed = sentence_embedding(col_name, spacy_model, fastText_model)\n",
    "        header_embedding = np.vstack((header_embedding, col_embed))\n",
    "    return header_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_Xy(question_embedding, header_embedding, sel_ind, conds_ind):\n",
    "    question_embed = 600\n",
    "    question_X = np.empty((0, question_embed))\n",
    "    question_y = np.empty((0,1))\n",
    "    for header_ind in range(len(header_embedding)):\n",
    "        question_X = np.vstack((question_X, np.concatenate((question_embedding, header_embedding[header_ind]))))\n",
    "        if header_ind == sel_ind or header_ind == conds_ind:\n",
    "            question_y = np.vstack((question_y, np.array(1)))\n",
    "        else:\n",
    "            question_y = np.vstack((question_y, np.array(0)))\n",
    "    return question_X, question_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_Xy(question_list, table_dict, spacy_model, fastText_model):\n",
    "    question_embed = 600\n",
    "    aggreation_embed = 300\n",
    "    question_Xs = np.empty((0, question_embed))\n",
    "    question_ys = np.empty((0,1))\n",
    "    aggreation_Xs = np.empty((0, aggreation_embed))\n",
    "    aggregation_ys = np.empty((0,1))\n",
    "    for question in question_list:\n",
    "        question_embedding = sentence_embedding(question['question'], spacy_model, fastText_model)\n",
    "        header_embedding = headers_embedding(table_dict[question['table_id']]['header'], spacy_model, \\\n",
    "                                           fastText_model)\n",
    "        \n",
    "        question_X, question_y = question_Xy(question_embedding, header_embedding, question['sql']['sel'], \\\n",
    "                                                  question['sql']['conds'][0][0])\n",
    "        question_Xs = np.vstack((question_Xs, question_X))\n",
    "        question_ys = np.vstack((question_ys, question_y))\n",
    "        \n",
    "        aggreation_Xs = np.vstack((aggreation_Xs, question_embedding))\n",
    "        if question['sql']['agg'] == 5:\n",
    "            agg = 0\n",
    "        else:\n",
    "            agg = 1\n",
    "        aggregation_ys = np.vstack((aggregation_ys, np.array(agg)))\n",
    "        \n",
    "    return question_Xs, question_ys, aggreation_Xs, aggregation_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "fastText = load_vectors('wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = load_questions('vis_train_questions.txt')\n",
    "test_questions = load_questions('vis_test_questions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables = load_tables('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tables = load_tables('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_train_X, column_train_y, agg_train_X, agg_train_y = obtain_Xy(train_questions, train_tables, nlp, fastText)\n",
    "column_test_X, column_test_y, agg_test_X, agg_test_y = obtain_Xy(test_questions, test_tables, nlp, fastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_clf = RandomForestClassifier()\n",
    "agg_clf.fit(agg_train_X,agg_train_y)\n",
    "agg_y_pred = agg_clf.predict(agg_test_X)\n",
    "accuracy_score(agg_test_y, agg_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_clf = MLPClassifier(hidden_layer_sizes=(256,128))\n",
    "agg_clf.fit(agg_train_X,agg_train_y)\n",
    "agg_y_pred = agg_clf.predict(agg_test_X)\n",
    "accuracy_score(agg_test_y, agg_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2\n"
     ]
    }
   ],
   "source": [
    "col_clf = RandomForestClassifier()\n",
    "col_clf.fit(column_train_X, column_train_y)\n",
    "\n",
    "# test_questions = questions[50:]\n",
    "test_tot = len(test_questions)\n",
    "correct_cnt = 0\n",
    "for question in test_questions:\n",
    "    col_test_X, col_test_y, _, _ = obtain_Xy([question], test_tables, nlp, fastText)\n",
    "    col_y_pred = col_clf.predict(col_test_X)\n",
    "    acc = accuracy_score(col_test_y, col_y_pred)\n",
    "    if acc == 1.0:\n",
    "        correct_cnt+=1\n",
    "print(\"accuracy:\", correct_cnt/test_tot)\n",
    "# np.intersect1d(np.where(column_test_y==1)[0], np.where(col_y_pred==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732620320855615"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_y_pred = col_clf.predict(column_test_X)\n",
    "accuracy_score(column_test_y, column_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "col_clf = MLPClassifier(hidden_layer_sizes=(512,256,64))\n",
    "# col_clf = RandomForestClassifier()\n",
    "col_clf.fit(column_train_X, column_train_y)\n",
    "\n",
    "# test_questions = questions[50:]\n",
    "test_tot = len(test_questions)\n",
    "correct_cnt = 0\n",
    "for question in test_questions:\n",
    "    col_test_X, col_test_y, _, _ = obtain_Xy([question], test_tables, nlp, fastText)\n",
    "    col_y_pred = col_clf.predict(col_test_X)\n",
    "    acc = accuracy_score(col_test_y, col_y_pred)\n",
    "    if acc == 1.0:\n",
    "        correct_cnt+=1\n",
    "print(\"accuracy:\", correct_cnt/test_tot)\n",
    "# np.intersect1d(np.where(column_test_y==1)[0], np.where(col_y_pred==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7379679144385026"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_y_pred = col_clf.predict(column_test_X)\n",
    "accuracy_score(column_test_y, column_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(column_test_y==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(col_y_pred==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(col_y_pred==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_test_X, column_test_y, _, _ = obtain_Xy(questions[50:51], tables, nlp, fastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'header': ['Position',\n",
       "  'Team',\n",
       "  'Points',\n",
       "  'Played',\n",
       "  'Drawn',\n",
       "  'Lost',\n",
       "  'Against',\n",
       "  'Difference'],\n",
       " 'page_title': '1911 in Brazilian football',\n",
       " 'types': ['real', 'text', 'real', 'real', 'real', 'real', 'real', 'text'],\n",
       " 'page_id': 15418823,\n",
       " 'id': '2-15418823-1',\n",
       " 'section_title': 'Campeonato Paulista',\n",
       " 'rows': [['1', 'São Paulo Athletic', '15', '9', '1', '1', '15', '8'],\n",
       "  ['2', 'Americano-SP', '11', '8', '1', '2', '15', '14'],\n",
       "  ['3', 'Paulistano', '7', '9', '1', '5', '15', '2'],\n",
       "  ['4', 'Germânia', '6', '9', '0', '6', '26', '- 12'],\n",
       "  ['5', 'Ypiranga-SP', '5', '9', '1', '6', '28', '- 15']],\n",
       " 'caption': 'Campeonato Paulista'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_id = '2-15418823-1'\n",
    "train_tables[tab_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
